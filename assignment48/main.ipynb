{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (1347, 10), (450, 64), (450, 10))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits=load_digits()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(digits.data,np.eye(10)[digits.target])\n",
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example\n",
    "### layer_hidden_length=[128,17]\n",
    "\n",
    "\n",
    "it's means we have 3 layers , layer1 has 128 neron and layer2 has 17 neron.\n",
    "\n",
    "⚠  for make object of this class we should have a layerhidden with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Neural_network():\n",
    "    def __init__(self,feathers_length,layer_hidden_length,classes_length):\n",
    "        self.w1=np.random.rand(feathers_length,layer_hidden_length[0])\n",
    "        self.b1=np.random.rand(layer_hidden_length[0])\n",
    "        self.w2=np.random.rand(layer_hidden_length[0],layer_hidden_length[1])\n",
    "        self.b2=np.random.rand(layer_hidden_length[1])\n",
    "        self.w3=np.random.rand(layer_hidden_length[1],classes_length)\n",
    "        self.b3=np.random.rand(classes_length)\n",
    "    def activation_function(self,type,X):\n",
    "        if type==\"sigmoid\":\n",
    "            return 1/(1+np.exp(-X))\n",
    "        elif type ==\"softmax\":\n",
    "            return np.exp(X)/np.sum(np.exp(X))\n",
    "    def fit(self,η,epochs,X_train,Y_train):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            Y_pred=[]\n",
    "            for x, y in zip(X_train, Y_train):\n",
    "                x = x.reshape(-1, 1)\n",
    "                # layer 1\n",
    "                net1 = x.T @ self.w1 + self.b1\n",
    "                out1 = self.activation_function(\"sigmoid\",net1)\n",
    "\n",
    "                # layer 2\n",
    "                net2 = out1 @ self.w2 + self.b2\n",
    "                out2 = self.activation_function(\"sigmoid\",net2)\n",
    "\n",
    "                # layer 3\n",
    "                net3 = out2 @ self.w3 + self.b3\n",
    "                out3 = self.activation_function(\"softmax\",net3)\n",
    "                \n",
    "                y_pred = out3\n",
    "                Y_pred.append(y_pred.T)\n",
    "                # back propagation\n",
    "                # layer 3\n",
    "                error = -2 * (y - y_pred)\n",
    "                grad_w3 = out2.T @ error\n",
    "                grad_b3 = error\n",
    "\n",
    "                # layer 2\n",
    "                error = error @ self.w3.T * out2 * (1 - out2)\n",
    "                grad_w2 = out1.T @ error\n",
    "                grad_b2 = error\n",
    "\n",
    "                # layer 1\n",
    "                error = error @ self.w2.T * out1 * (1 - out1)\n",
    "                grad_w1 = x @ error\n",
    "                grad_b1 = error\n",
    "\n",
    "                # update\n",
    "\n",
    "                # layer 1\n",
    "                # دست\n",
    "                \n",
    "                grad_b1=grad_b1.reshape(128)\n",
    "                grad_b2=grad_b2.reshape(32)\n",
    "                grad_b3=grad_b3.reshape(10)\n",
    "                self.w1 -= η * grad_w1\n",
    "                self.b1 -=η * grad_b1\n",
    "        \n",
    "                # layer 2\n",
    "                self.w2 -=η * grad_w2\n",
    "                self.b2 -=η * grad_b2\n",
    "\n",
    "                # layer 3\n",
    "                self.w3 -= η * grad_w3\n",
    "                self.b3 -=η * grad_b3\n",
    "    def predict(self,X):\n",
    "        Y_pred=[]\n",
    "        for x in X:\n",
    "            x = x.reshape(-1, 1)\n",
    "            # layer 1\n",
    "            net1 = x.T @ self.w1 + self.b1\n",
    "            out1 = self.activation_function(\"sigmoid\",net1)\n",
    "            # layer 2\n",
    "            net2 = out1 @ self.w2 + self.b2\n",
    "            out2 = self.activation_function(\"sigmoid\",net2)\n",
    "            # layer 3\n",
    "            net3 = out2 @ self.w3 + self.b3\n",
    "            out3 = self.activation_function(\"softmax\",net3)\n",
    "            \n",
    "            y_pred = out3\n",
    "            Y_pred.append(y_pred.T)\n",
    "        return Y_pred\n",
    "network=Neural_network(64,[128,32],10)\n",
    "network.fit(0.01,100,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurcy: 0.09111111111111111\n"
     ]
    }
   ],
   "source": [
    "Y_pred=network.predict(X_test)\n",
    "Y_pred=np.array(Y_pred).reshape(-1,10)\n",
    "print(\"accurcy:\",np.mean(np.argmax(Y_pred,axis=1)==np.argmax(Y_test,axis=1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
